{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd2368a-b147-46d6-8f72-7b6e3b32da70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820e669d-7c04-4a3c-8684-f4b43c9cd73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(dataset,col_name, output_array=False):\n",
    "    \"\"\"Standard Z score scaler\"\"\"\n",
    "    array = dataset[col_name].to_numpy()\n",
    "    mean = np.mean(array)\n",
    "    std = np.std(array)\n",
    "    scaled = (array-mean)/std\n",
    "    if output_array:\n",
    "        return scaled\n",
    "    else:\n",
    "        return scaled, mean, std "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86346c06-ea3f-4c11-a94c-e841c0276f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def series_to_supervised(data, n_in=1, n_out=1, variable_num = 0,  dropnan=True):\n",
    "    \"\"\"\n",
    "    Frame a time series as a supervised learning dataset.\n",
    "    Arguments:\n",
    "    data: Sequence of observations as a list or NumPy array.\n",
    "    n_in: Number of lag observations as input (X).\n",
    "    n_out: Number of observations as output (y).\n",
    "    dropnan: Boolean whether or not to drop rows with NaN values.\n",
    "    Returns:\n",
    "    Pandas DataFrame of series framed for supervised learning.\n",
    "    \"\"\"\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    \n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names+=[('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    \n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df[variable_num].shift(-i))\n",
    "        if i == 0:\n",
    "            names.append('var1(t)')\n",
    "        else:\n",
    "            names.append('var%d(t+%d)' % (1, i))\n",
    "    \n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg, names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d240e3e-abe9-4c6a-b12a-4401552990ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_date(array, lockdown=False):\n",
    "    \"\"\" Choose a random date within lockdown or out of lockdown.\"\"\"\n",
    "    if lockdown:\n",
    "        return np.random.choice(array[(array > pd.to_datetime('2020-03-24')) & (array < pd.to_datetime('2020-06-23')) ])\n",
    "    else:\n",
    "        #ensure a years worth of hourly data is available for training.\n",
    "        array = pd.to_datetime(array[(24*365):])\n",
    "        \n",
    "        # 2020-01-01 ensures only non-lockdown entries will be considered\n",
    "        return np.random.choice(array[array < pd.to_datetime('2020-01-01')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff31d60-8042-433a-840d-d22a779a72c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_date( dataset , reframed_data, date, days_prior=60):\n",
    "    \n",
    "    \"\"\" Split reframed data by specific date, days refers to the number \n",
    "    of days after the specified date the test set will contain.\n",
    "    days prior refers to the number of days used to train the model \"\"\"\n",
    "    \n",
    "    #get values from reframed data\n",
    "    values = reframed_data.values\n",
    "    \n",
    "    #convert string into datetime\n",
    "    date = pd.to_datetime(date)\n",
    "    \n",
    "    #create new dataframe\n",
    "    df = dataset.reset_index().rename(columns = {'index':'date'})\n",
    "    \n",
    "    #ensure dates are in datetime format\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    \n",
    "    #find index for date\n",
    "    date1_index = df.index[df['date'] == date1 ][0]\n",
    "    \n",
    "    #set train values to be all data upto given date \n",
    "    train = values[ date1_index - n_hours - (24*days_prior) : date1_index - n_hours, :]\n",
    "    \n",
    "    #remove training dataset so that tests can be done on the remaining datapoints \n",
    "    leftover_before = values[ : date1_index - n_hours - (24 * days_prior) , :]\n",
    "    leftover_after = values[ date1_index - n_hours : , :]\n",
    "\n",
    "    #get index of second date is 1 month into future\n",
    "    date2_index = date1_index + (30 * 24)\n",
    "\n",
    "    #test set two months after specified date\n",
    "    test = values[ ( date1_index - n_hours ) : (date2_index - n_hours)  + 1 , :]\n",
    "    \n",
    "    #get specific dates for test period\n",
    "    dates = df.date.to_numpy()\n",
    "    specific_dates = dates[ date1_index  : date2_index+1  ]\n",
    "    \n",
    "    if leftover_before.shape[0] > leftover_after.shape[0]:\n",
    "        \n",
    "        return train , test, specific_dates, leftover_before\n",
    "    \n",
    "    else:\n",
    "        return train , test, specific_dates, leftover_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e939eb9-30dd-4f0e-a8d8-daadc3391533",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape(data, n_features):\n",
    "    \"\"\" Reshape puts data in apprpriate format for LSTM usage\n",
    "    [samples, timesteps, features] \"\"\"\n",
    "    n_obs = n_hours * n_features\n",
    "\n",
    "    #split according to lag columns and output\n",
    "    data_X, data_y = data[:, :n_obs], data[:, -hours_after:]\n",
    "         \n",
    "    # reshape input to be 3D [samples, timesteps, features]\n",
    "    data_X_r = data_X.reshape((data_X.shape[0], n_hours , n_features))\n",
    "\n",
    "    return data_X, data_y , data_X_r "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205b4e5d-60a3-414e-ae2f-e46136aa3b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_scale(array,mean,std):\n",
    "    \"\"\" Inversion of Standard scaler \"\"\"\n",
    "    inverse = (array*std) + mean \n",
    "    return inverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38f4509-b301-4a80-be37-9ef288bfe8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_model(cells,dropout):\n",
    "    # design network\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(cells, activation = 'relu',dropout=dropout,input_shape=(train_X_r.shape[1], train_X_r.shape[2])))\n",
    "    model.add(Dense(n_hours, activation = 'relu'))\n",
    "    model.add(Dense(n_hours_out))\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['mse'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bd6e87-1ca7-4a23-abb5-1f4ffe1a70a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hour_predictor(test_r, index , timesteps):\n",
    "    \"\"\" Rolling one hour prediction: uses previous \n",
    "    1 hour predictions to predict 1 hour ahead \"\"\"\n",
    "    \n",
    "    #starting sequence used to get first prediction. \n",
    "    start = test_r[index].reshape(1,n_hours,n_features)\n",
    "    \n",
    "    #empty array to store results \n",
    "    results = []\n",
    "    \n",
    "    #get first prediction\n",
    "    result = model.predict(start)[0][0]\n",
    "    \n",
    "    #record it \n",
    "    results.append(result)\n",
    "    \n",
    "    for i in range (1,timesteps):\n",
    "\n",
    "        #shift window one time step\n",
    "        start = np.delete(start[0],0,0)\n",
    "        next_ts = test_r[index+i][2]\n",
    "        start = np.append(start,next_ts).reshape(1,n_hours,n_features)\n",
    "\n",
    "        #add prediction\n",
    "        start[0][n_hours-1][0] = result\n",
    "\n",
    "        #make new prediction\n",
    "        result = model.predict(start)[0][0]\n",
    "\n",
    "        results.append(result)\n",
    "        \n",
    "    return np.array(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7572924-e1a1-4ec6-87a0-2268eb99fffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "\n",
    "dataset = pd.read_csv('./MCC_combined.csv', header=0, index_col=0)\n",
    "\n",
    "#conver days column into one-hot-encoded variable\n",
    "days =['Mon','Tue','Wed','Thurs','Fri','Sat','Sun']\n",
    "data = pd.get_dummies(dataset['day_of_week'])\n",
    "data.columns = days\n",
    "dataset  = dataset.join(data)\n",
    "\n",
    "#get all dates\n",
    "all_dates = pd.to_datetime(dataset.index.to_numpy())\n",
    "\n",
    "\n",
    "#scale continuous variables --> ['temp', 'ws', 'wd', 'Volume']\n",
    "continuous_vars = ['temp', 'ws', 'wd', 'Volume']\n",
    "for i in continuous_vars:\n",
    "    dataset[i] = scale(dataset,i,output_array=True)\n",
    "    \n",
    "# scale NO2 keep mean and std for inverse scaling\n",
    "no2, mean, std, = scale(dataset,'NO2',output_array=False)\n",
    "dataset['NO2'] = no2\n",
    "\n",
    "#chose variables\n",
    "\n",
    "#-----NO2 only \n",
    "# ['NO2']\n",
    "\n",
    "#-----NO2 + traffic volume\n",
    "\n",
    "#['NO2','Volume']\n",
    "\n",
    "\n",
    "#------NO2 + meteorological\n",
    "\n",
    "#['NO2', 'ws' ,'temp','wd_1', 'wd_2', 'wd_3', 'wd_4']\n",
    "\n",
    "\n",
    "#------ ALL VARIABLES\n",
    "\n",
    "#['NO2', 'ws' ,'temp','wd_1', 'wd_2', 'wd_3', 'wd_4', 'Volume','Mon', 'Tue', 'Wed',\n",
    "       #'Thurs', 'Fri', 'Sat', 'Sun']\n",
    "\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7276fa59-adf0-4188-83eb-6c21c9b51ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paramter ranges\n",
    "HOURS_BEFORE = [24]\n",
    "BATCH_SIZE = [64]\n",
    "CELLS = [24]\n",
    "DROPOUT= [0]\n",
    "EPOCHS = [50]\n",
    "TRAIN_SIZE = [30]\n",
    "VARIABLES = [['NO2'],['NO2','Volume'], \n",
    "            ['NO2', 'ws' ,'temp','wd_1', 'wd_2', 'wd_3', 'wd_4'],\n",
    "            ['NO2', 'ws' ,'temp','wd_1', 'wd_2', 'wd_3', 'wd_4', 'Volume','Mon', 'Tue', 'Wed',\n",
    "             'Thurs', 'Fri', 'Sat', 'Sun']]\n",
    "TIMESTEPS = 1 # for rolling prediction\n",
    "LOCKDOWN = False\n",
    "rdm_date = False\n",
    "plot_days = 30\n",
    "\n",
    "\n",
    "if rdm_date:\n",
    "    #choose a random date \n",
    "    DATE = random_date(all_dates,lockdown=LOCKDOWN)\n",
    "else:\n",
    "    DATE =  '2020-03-24' #First lockdown\n",
    "    DATE1 = '2020-02-24' #Month Before Lockdown\n",
    "    \n",
    "results = []\n",
    "\n",
    "print ('runs:', len(HOURS_BEFORE)*len(BATCH_SIZE)\n",
    "      *len(CELLS)* len(DROPOUT) * len(EPOCHS) * len(TRAIN_SIZE)\n",
    "      * len(VARIABLES)* 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a023ae-71df-4f2b-9f30-cbcd8655d4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #create new dataframe\n",
    "# df = dataset.reset_index().rename(columns = {'index':'date'})\n",
    "\n",
    "# #ensure dates are in datetime format\n",
    "# df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# #find index for date\n",
    "# df[df['date'] >= DATE ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c31ea6-dafa-49cd-94a6-7fbe646d1e2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#-----------UPDATE PREDICTOR-----------\n",
    "count = 0 \n",
    "#---specify bounds of lag and prediction\n",
    "for h in HOURS_BEFORE:\n",
    "    for b in BATCH_SIZE:\n",
    "        for c in CELLS:\n",
    "            for d in DROPOUT:\n",
    "                for e in EPOCHS:\n",
    "                    for t in TRAIN_SIZE:\n",
    "                        for v in VARIABLES:\n",
    "                            \n",
    "                            print (f\"hb:{h}, ts:{t}, e:{e}, bs:{b}, c:{c}, vars:{len(v)}  \")\n",
    "                            \n",
    "                            dataset_copy = dataset[v]\n",
    "                            \n",
    "                            hours_before = h\n",
    "                            hours_after = TIMESTEPS\n",
    "\n",
    "                            days_before = 1\n",
    "                            days_after = 1\n",
    "\n",
    "                            # # specify the number of lag hours\n",
    "                            n_hours = hours_before * days_before # lag hours for each feature (t-x)\n",
    "                            n_features = len(v) # number of features \n",
    "                            n_hours_out = hours_after * days_after # lenghth of output forecast (<1 == <t+1)\n",
    "\n",
    "                            date1 = DATE\n",
    "\n",
    "                            #convert values\n",
    "                            values = dataset_copy.values.astype('float32') \n",
    "\n",
    "                            # frame as supervised learning\n",
    "                            reframed, names = series_to_supervised(values, n_hours, n_hours_out )\n",
    "\n",
    "\n",
    "                            train , test,  dates , leftover = split_by_date(dataset_copy, reframed, date1, t)\n",
    "\n",
    "#                             print(train.shape, test.shape, dates.shape)\n",
    "\n",
    "                            train_X, train_y , train_X_r = reshape(train,n_features)\n",
    "                            test_X, test_y , test_X_r = reshape(test,n_features)\n",
    "\n",
    "#                             print( train_X_r.shape, test_X_r.shape)\n",
    "\n",
    "                            for rep in range (3):\n",
    "\n",
    "                                model = LSTM_model(c,d)\n",
    "\n",
    "                                # fit network\n",
    "                                history = model.fit(train_X_r, train_y, epochs=e, batch_size=b, validation_split = 0.2 , verbose=0, shuffle=False)\n",
    "\n",
    "                                # get scaled prediction and observed values to calculate RMSE\n",
    "                                predicted = model.predict(test_X_r)[::n_hours_out]\n",
    "                                observed = test_y[::n_hours_out]\n",
    "\n",
    "                                #calculate RMSE\n",
    "                                RMSE = mean_squared_error(observed, predicted,squared = False)\n",
    "\n",
    "                                # rescale for plotting\n",
    "                                predicted_rescaled = inverse_scale(predicted.flatten(), mean, std)[:dates.shape[0]]\n",
    "                                observed_rescaled = inverse_scale(observed.flatten(), mean, std)[:dates.shape[0]]\n",
    "                                \n",
    "                                MAPE = mean_absolute_percentage_error(observed_rescaled,predicted_rescaled)\n",
    "\n",
    "\n",
    "                            #     #calculate mean RMSE for each timestep \n",
    "                            #     lag_scores = np.array([mean_squared_error(observed_update[i],predicted_update[i], squared=False ) for i in range(observed_update.shape[0])])\n",
    "                            #     lag_scores = np.mean(lag_scores.reshape(int(observed_update.shape[0]/TIMESTEPS),TIMESTEPS),axis=0)\n",
    "\n",
    "                                                  \n",
    "                                results.append([h,t,e, b, c, len(v) ,RMSE, MAPE] )\n",
    "                                count +=1 \n",
    "                                print(f'{count}, {rep}, RMSE:{RMSE}, MAPE: {MAPE} ')\n",
    "                                #plot \n",
    "                                if rep == 2: \n",
    "                                    plt.figure(figsize = (15,8))\n",
    "                                    plt.plot(dates[:24*plot_days],observed_rescaled[:24*plot_days],label='Observed')\n",
    "                                    plt.plot(dates[:24*plot_days],predicted_rescaled[:24*plot_days],label='Predicted')\n",
    "                                    plt.legend()\n",
    "                                    plt.xticks(rotation=45)\n",
    "                                    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e820ec5c-7d0c-4c38-acf8-b140177eddb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d3cc02-83f3-48d5-9333-a3b527d60ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ----- ROLLING 1 HOUR PREDICTOR---------\n",
    "# TIMESTEPS = 3\n",
    "\n",
    "# #specify bounds of lag and prediction\n",
    "# hours_before = 24\n",
    "# hours_after = 1\n",
    "\n",
    "# days_before = 1\n",
    "# days_after = 1\n",
    "\n",
    "# # # specify the number of lag hours\n",
    "# n_hours = hours_before * days_before # lag hours for each feature (t-x)\n",
    "# n_features = len(dataset.columns) # number of features \n",
    "# n_hours_out = hours_after * days_after # lenghth of output forecast (<1 == <t+1)\n",
    "\n",
    "# date1 = DATE\n",
    "\n",
    "# #convert values\n",
    "# values = dataset.values.astype('float32') \n",
    "\n",
    "# # frame as supervised learning\n",
    "# reframed, names = series_to_supervised(values, n_hours, n_hours_out )\n",
    "\n",
    "# print(f'1h rolling Model will use last {n_hours} hours from {list(dataset.columns)} variables to predict {TIMESTEPS} ahead.')\n",
    "\n",
    "# train , test,  dates , leftover = split_by_date( dataset, reframed, date1, TRAIN_SIZE )\n",
    "\n",
    "\n",
    "# print ( dates.shape, test.shape ) \n",
    "\n",
    "# #print(train.shape, test.shape, dates.shape)\n",
    "\n",
    "# train_X, train_y , train_X_r = reshape(train,n_features)\n",
    "# test_X, test_y , test_X_r = reshape(test,n_features)\n",
    "\n",
    "# print( train_X_r.shape, test_X_r.shape)\n",
    "\n",
    "\n",
    "# for rep in range (1):\n",
    "\n",
    "#     model = LSTM_model(CELLS,DROPOUT)\n",
    "\n",
    "#     # fit network\n",
    "#     history = model.fit(train_X_r, train_y, epochs = EPOCHS, batch_size=BATCH_SIZE, validation_split = 0.2 , verbose=0, shuffle=False)\n",
    "\n",
    "#     #timesteps = how far ahead the rolling 1 hour predictor will forcast. \n",
    "#     # array of indexes  \n",
    "#     indexes = np.arange(0,test_X_r.shape[0] - TIMESTEPS , TIMESTEPS)\n",
    "\n",
    "#     # use rolling hour predictor to predict x timesteps ahead at each index within the array to get \n",
    "#     # contiguous predictions over a longer timeframe.\n",
    "#     predicted = np.array([hour_predictor(test_X_r, i,  TIMESTEPS) for i in indexes]).flatten()\n",
    "    \n",
    "#     observed = test_y.flatten()[:predicted.shape[0]]\n",
    "\n",
    "#     #invert scale and reshape for evaluation\n",
    "#     predicted_rescaled = inverse_scale( predicted.reshape(len(predicted),1), mean, std )\n",
    "\n",
    "#     #invert scale observed values, trim observed values so that predicted and observed have equal sizes and shape\n",
    "#     observed_rescaled = inverse_scale(test_y.flatten()[:predicted.shape[0]],mean,std).reshape(predicted.shape[0],1)\n",
    "\n",
    "#     #calculate RMSE\n",
    "#     total_score = mean_squared_error(observed,predicted,squared = False)\n",
    "\n",
    "# #     #calculate mean RMSE for each timestep \n",
    "# #     lag_scores = np.array([mean_squared_error(observed[i],predicted[i], squared=False ) for i in range(observed.shape[0])])\n",
    "# #     lag_scores = np.mean(lag_scores.reshape(int(observed.shape[0]/TIMESTEPS),TIMESTEPS),axis=0)\n",
    "\n",
    "# #     print (f\"Rolling Predictor: Epochs: {EPOCHS}, Batch Size: {BATCH_SIZE}, TIMESTEP:{TIMESTEPS}, RMSE:{total_score}, Lag Scores {lag_scores}\")\n",
    "    \n",
    "#     print ( total_score )\n",
    "#     #trim dates \n",
    "#     #dates = dates[:predicted.shape[0]]\n",
    "#     plt.figure(figsize = (15,8))\n",
    "#     plt.plot(dates[:24*plot_days],observed_rescaled[:24*plot_days],label='Observed')\n",
    "#     plt.plot(dates[:24*plot_days],predicted_rescaled[:24*plot_days],label='Predicted')\n",
    "#     plt.legend()\n",
    "#     plt.xticks(rotation=45)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cf144d-d9eb-4010-892a-ab18a8f2a436",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77ed34b-c527-4e9c-b5da-52bec3923d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def averager(array):\n",
    "#     before = 0\n",
    "#     after = array.shape[0]-1\n",
    "#     results = []\n",
    "#     for i in range(array.shape[0]):\n",
    "\n",
    "#         before_array = np.zeros(before)\n",
    "#         before_array[:] = np.nan\n",
    "\n",
    "#         after_array = np.zeros(after)\n",
    "#         after_array[:] = np.nan\n",
    "\n",
    "#         results.append(np.concatenate((before_array, array[i], after_array)))\n",
    "\n",
    "#         before = before + 1  \n",
    "#         after= after - 1 \n",
    "\n",
    "#     results = np.vstack(results)\n",
    "    \n",
    "#     return np.nanmean(results,axis=0), np.nanstd(results,axis=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
